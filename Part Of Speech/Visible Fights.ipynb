{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk \n",
    "import operator\n",
    "import math\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_POS = []\n",
    "f = open('/home/rahmtin/Documents/Arxiv-Feynmann/Arxiv/Part Of Speech/POS.txt')\n",
    "for line in f:\n",
    "    global_POS.append(line[0:2])\n",
    "\n",
    "global_POS = list(set(global_POS))\n",
    "relevant_features = ['VB', 'NN', 'RB', 'JJ', 'DT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Paper:\n",
    "    def __init__(self, title):\n",
    "         self.title = title\n",
    "         self.authors = []\n",
    "         self.experiences = []\n",
    "         self.year = ''\n",
    "         self.pos = []\n",
    "         self.features = dict()\n",
    "         self.tag = \"\"\n",
    "         \n",
    "    def to_string(self):\n",
    "        return [self.title, self.year, self.authors, self.features]\n",
    "    \n",
    "\n",
    "f = open('/home/rahmtin/Documents/Arxiv-Feynmann/Arxiv/Part Of Speech/TitlesAndAuthorsAndMonth4.txt')\n",
    "papers = []\n",
    "paper = Paper('test')\n",
    "counter = 0;\n",
    "global_features = [':', '?' , 'MATH']\n",
    "tag = \"\"\n",
    "author_papers = dict()\n",
    "\n",
    "for line in f:\n",
    "    try:\n",
    "        text = nltk.word_tokenize(line)\n",
    "    except UnicodeDecodeError:\n",
    "        for line2 in f:\n",
    "            if len(nltk.word_tokenize(line2)) == 0:\n",
    "                break\n",
    "        continue\n",
    "\n",
    "    if len(text) == 0:\n",
    "        papers.append(paper)\n",
    "        counter = counter + 1\n",
    "#        if counter >= 50:\n",
    "#            break\n",
    "        continue\n",
    "\n",
    "    if text[0] == 'Tag':\n",
    "        tag = text[2]\n",
    "        \n",
    "    if text[0] == 'Title':\n",
    "        title = (' '.join(text[2:])).lower().decode('utf-8', 'ignore').encode('utf-8', 'ignore')\n",
    "        number = 0\n",
    "        tmp = ''\n",
    "        for x in title:\n",
    "            if x == '$':\n",
    "                if number == 1:\n",
    "                    tmp = tmp + \" MATH \"\n",
    "                number = 1 - number\n",
    "                continue\n",
    "            if number > 0:\n",
    "                continue\n",
    "            tmp = tmp + x\n",
    "        paper = Paper(' '.join(tmp.split()))\n",
    "        paper.pos = nltk.pos_tag(nltk.word_tokenize(paper.title))\n",
    "        if len(paper.pos) > 0:\n",
    "            paper.pos[0] = (paper.pos[0][0], paper.pos[0][1][:2])\n",
    "        paper.tag = tag\n",
    "        for feature in global_features: \n",
    "            paper.features[feature] = 0\n",
    "            if feature in paper.title:\n",
    "                paper.features[feature] = 1\n",
    "            \n",
    "    \n",
    "    if text[0] == 'Year':\n",
    "        if text[2][0] == '9':\n",
    "            text[2] = '19' + text[2]\n",
    "        else:\n",
    "            text[2] = '20' + text[2]\n",
    "        paper.year = text[2]\n",
    "    if text[0] == 'Author':\n",
    "        author = ' '.join(text[2:]).lower()\n",
    "        paper.authors.append(' '.join(author.split()))\n",
    "    \n",
    "papers.sort(key=operator.attrgetter('year'))\n",
    "author_experience = dict()\n",
    "author_papers = dict()\n",
    "for index, paper in enumerate(papers):\n",
    "    for author in paper.authors:\n",
    "        if author not in author_experience:\n",
    "            author_experience[author] = 0\n",
    "            author_papers[author] = []\n",
    "        author_papers[author].append(index)\n",
    "        paper.experiences.append(author_experience[author])\n",
    "        author_experience[author] = author_experience[author] + 1\n",
    "\n",
    "#    print \"---->\", paper.authors, paper.year\n",
    "#    print paper.experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def find_scores(paper_list):\n",
    "    if(len(paper_list) < min_for_min_threshold):\n",
    "        sys.exit()\n",
    "    ret = dict()\n",
    "    for feature in global_features:\n",
    "        ret[feature] = 0\n",
    "    for feature in relevant_features:\n",
    "        ret[feature] = 0\n",
    "    for index in paper_list:\n",
    "        paper = papers[index]\n",
    "        for feature in global_features:\n",
    "            ret[feature] = ret[feature] + int(paper.features[feature])\n",
    "        for feature in relevant_features:\n",
    "            if len(paper.pos) > 0:\n",
    "                ret[feature] = ret[feature] + int(paper.pos[0][1] == feature)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1291\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#author_papers = dict()\n",
    "older_bigger_fights = []\n",
    "younger_bigger_fights = []\n",
    "fight_count = 0\n",
    "author_pairs = set()\n",
    "regression_scores = dict()\n",
    "regression_label = dict()\n",
    "denom = dict()\n",
    "num = dict()\n",
    "ratio = dict()\n",
    "first_pos_point = []\n",
    "\n",
    "min_for_min_threshold = 1\n",
    "max_for_min_threshold = 2\n",
    "final_experience_threshold_for_younger = 10\n",
    "\n",
    "for feature in global_features + relevant_features:\n",
    "    denom[feature] = dict()\n",
    "    num[feature] = dict()\n",
    "    ratio[feature] = dict()\n",
    "    regression_scores[feature] = []\n",
    "    regression_label[feature] = []\n",
    "    one_sided_regression_denom[feature] = dict()\n",
    "    one_sided_regression_num[feature] = dict()\n",
    "    \n",
    "for index, paper in enumerate(papers):\n",
    "    if len(paper.authors) == 2 and len(paper.pos) > 0:\n",
    "        if tuple(paper.authors) not in author_pairs:\n",
    "            older_index = younger_index = 0\n",
    "            if paper.experiences[0] < paper.experiences[1]: \n",
    "                older_index = 1\n",
    "            else:\n",
    "                younger_index = 1\n",
    "            \n",
    "            if(min(paper.experiences) >= min_for_min_threshold and min(paper.experiences) <= max_for_min_threshold and\n",
    "              paper.experiences[older_index] / paper.experiences[younger_index] > 2 and \n",
    "              paper.experiences[older_index] - paper.experiences[younger_index] > 10 and\n",
    "              author_experience[paper.authors[younger_index]] > final_experience_threshold_for_younger):\n",
    "                # we found a fight\n",
    "                fight_count += 1\n",
    "                older_papers = author_papers[paper.authors[older_index]]\n",
    "                younger_papers = author_papers[paper.authors[younger_index]]\n",
    "                temp_older_papers = older_papers\n",
    "                older_papers = list(set(older_papers) - set(younger_papers))\n",
    "                younger_papers = list(set(younger_papers) - set(temp_older_papers))\n",
    "                older_scores = find_scores(older_papers)\n",
    "                younger_scores = find_scores(younger_papers)\n",
    "                for feature in global_features + relevant_features:\n",
    "                    younger_score = younger_scores[feature]\n",
    "                    older_score = older_scores[feature]\n",
    "                    if (younger_score, older_score) not in denom[feature]:\n",
    "                        denom[feature][(younger_score, older_score)] = 0\n",
    "                        num[feature][(younger_score, older_score)] = 0\n",
    "                    denom[feature][(younger_score, older_score)] = denom[feature][(younger_score, older_score)] + 1\n",
    "                    regression_scores[feature].append([younger_score, older_score])\n",
    "                    younger_prob = float(younger_score) / len(younger_papers) \n",
    "                    older_prob = float(older_score) / len(older_papers)\n",
    "                    my_tuple = (younger_prob, min(paper.experiences),\n",
    "                                older_prob, max(paper.experiences))\n",
    "                    \n",
    "                    if my_tuple not in one_sided_regression_denom[feature]: \n",
    "                        one_sided_regression_denom[feature][my_tuple] = 0\n",
    "                        one_sided_regression_num[feature][my_tuple] = 0\n",
    "                    one_sided_regression_denom[feature][my_tuple] += 1    \n",
    "                    \n",
    "                    if feature in global_features:\n",
    "                        regression_label[feature].append(paper.features[feature])\n",
    "                        if paper.features[feature] == 0:\n",
    "                            my_tuple = my_tuple + (0,)\n",
    "                            continue\n",
    "                        num[feature][(younger_score, older_score)] = num[feature][(younger_score, older_score)] + 1\n",
    "                        one_sided_regression_num[feature][my_tuple] += 1\n",
    "                        my_tuple = my_tuple + (1,)\n",
    "                   \n",
    "                    if feature in relevant_features:\n",
    "                        regression_label[feature].append(int(paper.pos[0][1] == feature))\n",
    "                        num[feature][(younger_score, older_score)] = (num[feature][(younger_score, older_score)] +\n",
    "                                                                      int(paper.pos[0][1] == feature))\n",
    "                        one_sided_regression_num[feature][my_tuple] += int(paper.pos[0][1] == feature)\n",
    "                        my_tuple = my_tuple + (int(paper.pos[0][1] == feature),)\n",
    "                    if older_prob > younger_prob:\n",
    "                        older_bigger_fights.append(my_tuple)\n",
    "                    elif younger_prob > older_prob:\n",
    "                        younger_bigger_fights.append(my_tuple)\n",
    "                  \n",
    "                                \n",
    "    for author1 in paper.authors:\n",
    "        for author2 in paper.authors:\n",
    "            author_pairs.add((author1, author2))\n",
    "            author_pairs.add((author2, author1))\n",
    "#        if author1 not in author_papers:\n",
    "#            author_papers[author1] = []\n",
    "#        author1_papers = author_papers[author1]\n",
    "#        while len(author1_papers) >= min_for_min_threshold:\n",
    "#            author1_papers.pop(0)\n",
    "#        author1_papers.append(index)\n",
    "#        author_papers[author1] = author1_papers\n",
    "\n",
    "print fight_count\n",
    "for feature in global_features + relevant_features:\n",
    "    copy1 = dict()\n",
    "    copy2 = dict()\n",
    "    for my_tuple in denom[feature]:\n",
    "        denom[feature][my_tuple] = max(1, denom[feature][my_tuple])\n",
    "        ratio[feature][my_tuple] =  (num[feature][my_tuple] / float(denom[feature][my_tuple]))\n",
    "        (i,j) = my_tuple\n",
    "        if ((j,i) not in denom[feature]):\n",
    "            copy1[(j, i)] = 1\n",
    "            copy2[(j, i)] = 0\n",
    "    denom[feature].update(copy1)\n",
    "    num[feature].update(copy2)\n",
    "    ratio[feature].update(copy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3352 2779\n",
      "1077 448 629 3.85611303246e-08\n"
     ]
    }
   ],
   "source": [
    "#match younger_bigger_fights with older_bigger_fights\n",
    "import copy\n",
    "print len(older_bigger_fights), len(younger_bigger_fights)\n",
    "matches = 0\n",
    "older_fights = copy.copy(older_bigger_fights)\n",
    "younger_fights = copy.copy(younger_bigger_fights)\n",
    "younger_won_duel = 0\n",
    "older_won_duel = 0\n",
    "\n",
    "for older_fight in older_fights: \n",
    "    for younger_fight in younger_fights:\n",
    "        if ((abs(younger_fight[0] - older_fight[2]) < 1e-2) and (abs(younger_fight[2] - older_fight[0]) < 1e-2) and\n",
    "            abs(younger_fight[4] - older_fight[4]) == 1): # the probs are pretty close and different outcomes\n",
    "            matches += 1\n",
    "     #       print younger_fight[4], older_fight[4]\n",
    "            if younger_fight[4] == 1:\n",
    "                younger_won_duel += 1\n",
    "            else:\n",
    "                older_won_duel += 1\n",
    "            younger_fights.remove(younger_fight)\n",
    "            break\n",
    "print matches, younger_won_duel, older_won_duel, scipy.stats.binom_test(older_won_duel, matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature in relevant_features + global_features:\n",
    "    for (i,j) in num[feature]:\n",
    "        if i < j: # i is younger, j is older\n",
    "        #(younger less usage, older more usage) - (older less usage, younger more usage)\n",
    "                    \n",
    "            p_value = scipy.stats.binom_test(num[feature][(j,i)], denom[feature][(j,i)], ratio[feature][(i,j)], alternative='two-sided')\n",
    "               \n",
    "            mean1 = ratio[feature][(i,j)]\n",
    "            var1 = mean1 * (1 - mean1)\n",
    "                #print mean1, (1 - mean1), var1\n",
    "            mean2 = ratio[feature][(j,i)]\n",
    "            var2 = mean2 * (1 - mean2)\n",
    "                #print mean2, (1 - mean2), var2\n",
    "            sample_numbers = denom[feature][(i,j)] + denom[feature][(j,i)]\n",
    "            CI = 1.96  * (var1 + var2) / (math.sqrt(denom[feature][(i,j)]) + math.sqrt(denom[feature][(j,i)]))\n",
    "    \n",
    "        #print (feature, i, j,\n",
    "        #       str(ratio[feature][(i,j)] - ratio[feature][(j,i)]) + \" +/- \" + str(CI), p_value,\n",
    "        #       num[feature][(i,j)], denom[feature][(i,j)],\n",
    "        #       num[feature][(j,i)], denom[feature][(j,i)] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## One sided/Both sided tuple experiment: regression\n",
    "from random import randrange\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "relevant_features = ['VB', 'NN', 'RB', 'JJ', 'DT']\n",
    "f = open('/home/rahmtin/Documents/Arxiv-Feynmann/Arxiv/DominantName/Regression.txt', 'w')\n",
    "def do_regression(data):\n",
    "    for feature in relevant_features + global_features:\n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        one_sided_regression_features = [[], []]\n",
    "        one_sided_regression_labels = [[], []]\n",
    "        for key in data[feature]:\n",
    "            all_features.append(key)\n",
    "            all_labels.append(data[feature][key])\n",
    "    \n",
    "        all_features = preprocessing.scale(all_features)\n",
    "        for index in range(len(all_features)):\n",
    "            random_index = randrange(0,2)\n",
    "            one_sided_regression_features[random_index].append(all_features[index])\n",
    "            one_sided_regression_labels[random_index].append(all_labels[index])\n",
    "    \n",
    "#    print one_sided_regression_features[0][0], one_sided_regression_features[1][0]\n",
    "#    print one_sided_regression_labels[0][0], one_sided_regression_labels[1][0]\n",
    "#    break\n",
    "        clf = linear_model.LinearRegression() # first one is younger, second one is older\n",
    "        clf.fit(one_sided_regression_features[0], one_sided_regression_labels[0])\n",
    "        prediction = clf.predict(one_sided_regression_features[1])\n",
    "        five_decimals = [\"%.5f\" % v for v in clf.coef_]\n",
    "        five_decimals_intercept = [\"%.5f\" % clf.intercept_]\n",
    "        print >>f, feature, ' Coefficients: ', five_decimals, five_decimals_intercept\n",
    "        print >>f, \"R^2 score: \", r2_score(one_sided_regression_labels[1], prediction)\n",
    "        \n",
    "\n",
    "one_sided_num_compact = dict()\n",
    "one_sided_denom_compact = dict()\n",
    "young_sided_regression_ratio = dict()\n",
    "old_sided_regression_ratio = dict()\n",
    "both_sided_regression_ratio = dict()\n",
    "relevant_features = ['VB', 'NN', 'RB', 'JJ', 'DT']\n",
    "\n",
    "for feature in global_features + relevant_features:\n",
    "    young_sided_regression_ratio[feature] = dict()\n",
    "    old_sided_regression_ratio[feature] = dict()\n",
    "    one_sided_num_compact[feature] = dict()\n",
    "    one_sided_denom_compact[feature] = dict()\n",
    "    both_sided_regression_ratio[feature] = dict()\n",
    "\n",
    "for feature in global_features + relevant_features:\n",
    "    for my_tuple in one_sided_regression_num[feature]:\n",
    "        # (young_prob, young_exp, old_prob, old_exp)\n",
    "        both_sided_regression_ratio[feature][my_tuple] = (one_sided_regression_num[feature][my_tuple] / \n",
    "                                                   max(one_sided_regression_denom[feature][my_tuple], 1))\n",
    "        compact_tuple = (my_tuple[0], my_tuple[1], my_tuple[3])\n",
    "        if compact_tuple not in one_sided_num_compact[feature]:\n",
    "            one_sided_num_compact[feature][compact_tuple] = 0\n",
    "            one_sided_denom_compact[feature][compact_tuple] = 0\n",
    "        one_sided_num_compact[feature][compact_tuple] += one_sided_regression_num[feature][my_tuple]\n",
    "        one_sided_denom_compact[feature][compact_tuple] += one_sided_regression_denom[feature][my_tuple]\n",
    "    for compact_tuple in one_sided_denom_compact[feature]:\n",
    "        young_sided_regression_ratio[feature][compact_tuple] = (one_sided_num_compact[feature][compact_tuple] / \n",
    "                                                    max(one_sided_denom_compact[feature][compact_tuple], 1))\n",
    "\n",
    "for feature in global_features + relevant_features:\n",
    "    one_sided_num_compact[feature] = dict()\n",
    "    one_sided_denom_compact[feature] = dict()\n",
    "    for my_tuple in one_sided_regression_num[feature]:\n",
    "        # (young_prob, young_exp, old_prob, old_exp)\n",
    "        compact_tuple = (my_tuple[2], my_tuple[3], my_tuple[1])\n",
    "        if compact_tuple not in one_sided_num_compact[feature]:\n",
    "            one_sided_num_compact[feature][compact_tuple] = 0\n",
    "            one_sided_denom_compact[feature][compact_tuple] = 0\n",
    "        one_sided_num_compact[feature][compact_tuple] += one_sided_regression_num[feature][my_tuple]\n",
    "        one_sided_denom_compact[feature][compact_tuple] += one_sided_regression_denom[feature][my_tuple]\n",
    "    for compact_tuple in one_sided_denom_compact[feature]:\n",
    "        old_sided_regression_ratio[feature][compact_tuple] = (one_sided_num_compact[feature][compact_tuple] / \n",
    "                                                    max(one_sided_denom_compact[feature][compact_tuple], 1))\n",
    "                                                 \n",
    "\n",
    "print >>f, \"OLD: (old_prob, old_experience, young_experience), intercept\"\n",
    "do_regression(old_sided_regression_ratio)\n",
    "print >>f, \"\"\n",
    "print >>f, \"YOUNG: (young_prob, young_experience, old_experience), intercept\"                                               \n",
    "do_regression(young_sided_regression_ratio)\n",
    "print >>f, \"\"\n",
    "print >>f, \"ALL: (young_prob, young_experience, old_prob, old_experience), intercept\"                      \n",
    "do_regression(both_sided_regression_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## One sided/Both sided tuple experiment: logistic regression\n",
    "from random import randrange\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "relevant_features = ['VB', 'NN', 'RB', 'JJ', 'DT']\n",
    "f = open('/home/rahmtin/Documents/Arxiv-Feynmann/Arxiv/DominantName/LogisticRegression.txt', 'w')\n",
    "def do_logistic_regression(data, labels):\n",
    "    for feature in relevant_features + global_features:\n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        one_sided_regression_features = [[], []]\n",
    "        one_sided_regression_labels = [[], []]\n",
    "        for index, key in enumerate(data[feature]):\n",
    "            all_features.append(data[feature][index])\n",
    "            all_labels.append(labels[feature][index])\n",
    "    \n",
    "        all_features = preprocessing.scale(all_features)\n",
    "        for index in range(len(all_features)):\n",
    "            random_index = randrange(0,2)\n",
    "            one_sided_regression_features[random_index].append(all_features[index])\n",
    "            one_sided_regression_labels[random_index].append(all_labels[index])\n",
    "    \n",
    "#    print one_sided_regression_features[0][0], one_sided_regression_features[1][0]\n",
    "#    print one_sided_regression_labels[0][0], one_sided_regression_labels[1][0]\n",
    "#    break\n",
    "        logistic = linear_model.LogisticRegression(C=1e5)\n",
    "        logistic.fit(one_sided_regression_features[0], one_sided_regression_labels[0])\n",
    "        LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
    "            fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "            multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
    "            solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
    "        \n",
    "        prediction = logistic.predict(one_sided_regression_features[1])\n",
    "        five_decimals = [\"%.5f\" % v for v in logistic.coef_[0]]\n",
    "        five_decimals_intercept = [\"%.5f\" % logistic.intercept_]\n",
    "        print >>f, feature, ' Coefficients: ', five_decimals, five_decimals_intercept\n",
    "        print >>f, \"Accuracy: \",  accuracy_score(one_sided_regression_labels[1], prediction)\n",
    "        \n",
    "\n",
    "\n",
    "young_features = dict()\n",
    "old_features = dict()\n",
    "both_features = dict()\n",
    "young_labels = dict()\n",
    "old_labels = dict()\n",
    "both_labels = dict()\n",
    "relevant_features = ['VB', 'NN', 'RB', 'JJ', 'DT']\n",
    "\n",
    "for feature in global_features + relevant_features:\n",
    "    young_features[feature] = []\n",
    "    old_features[feature] = []\n",
    "    both_features[feature] = []\n",
    "    young_labels[feature] = []\n",
    "    old_labels[feature] = []\n",
    "    both_labels[feature] = []\n",
    "    \n",
    "\n",
    "for feature in global_features + relevant_features:\n",
    "    for my_tuple in one_sided_regression_num[feature]:\n",
    "        # (young_prob, young_exp, old_prob, old_exp)\n",
    "        young_compact_tuple = (my_tuple[0], my_tuple[1], my_tuple[3])\n",
    "        old_compact_tuple = (my_tuple[2], my_tuple[3], my_tuple[1])\n",
    "        for i in range(one_sided_regression_denom[feature][my_tuple]):\n",
    "            label = 0\n",
    "            if i < one_sided_regression_num[feature][my_tuple]:\n",
    "                label = 1      \n",
    "            both_features[feature].append(my_tuple)\n",
    "            both_labels[feature].append(label)\n",
    "            young_features[feature].append(young_compact_tuple)\n",
    "            young_labels[feature].append(label)\n",
    "            old_features[feature].append(old_compact_tuple)\n",
    "            old_labels[feature].append(label)\n",
    "            \n",
    "                                                 \n",
    "\n",
    "print >>f, \"OLD: (old_prob, old_experience, young_experience), intercept\"\n",
    "do_logistic_regression(old_features, old_labels)\n",
    "print >>f, \"\"\n",
    "print >>f, \"YOUNG: (young_prob, young_experience, old_experience), intercept\"                                               \n",
    "do_logistic_regression(young_features, young_labels)\n",
    "print >>f, \"\"\n",
    "print >>f, \"ALL: (young_prob, young_experience, old_prob, old_experience), intercept\"                      \n",
    "do_logistic_regression(both_features, both_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(': Coefficients: \\n', array([ 0.04366242,  0.00843984]))\n",
      "('? Coefficients: \\n', array([ 0.04736798,  0.01052285]))\n",
      "('MATH Coefficients: \\n', array([ 0.11733052,  0.00641327]))\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import linear_model\n",
    "clf = linear_model.LinearRegression() # first one is younger, second one is older\n",
    "#print regression_scores[':']\n",
    "clf.fit(regression_scores[':'], regression_label[':'])\n",
    "print(': Coefficients: \\n', clf.coef_)\n",
    "clf.fit(regression_scores['?'], regression_label['?'])\n",
    "print('? Coefficients: \\n', clf.coef_)\n",
    "clf.fit(regression_scores['MATH'], regression_label['MATH'])\n",
    "print('MATH Coefficients: \\n', clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "younger, older wins:  781 761\n"
     ]
    }
   ],
   "source": [
    "\n",
    "older_wins_count = 0                                                                           \n",
    "younger_wins_count = 0\n",
    "for x in first_pos_point:\n",
    "    if x[0] < x[1]:\n",
    "        older_wins_count += 1\n",
    "    elif x[1] < x[0]:\n",
    "        younger_wins_count += 1\n",
    "\n",
    "print \"younger, older wins: \", younger_wins, older_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?r2_score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
